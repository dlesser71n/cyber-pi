#!/usr/bin/env python3
"""
ScraperAPI Setup and Testing Script
Configure and test ScraperAPI for dark web intelligence collection
"""

import os
import sys
import asyncio
import aiohttp
import json
import logging
from datetime import datetime

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def setup_scraperapi():
    """Setup ScraperAPI configuration"""
    print("üöÄ ScraperAPI Setup for Dark Web Intelligence Collection")
    print("=" * 60)
    
    # Check for API key
    api_key = os.getenv('SCRAPERAPI_KEY')
    if not api_key:
        print("\n‚ùå ScraperAPI key not found!")
        print("Please set your ScraperAPI key:")
        print("export SCRAPERAPI_KEY='your_api_key_here'")
        print("\nOr get a free key at: https://www.scraperapi.com/")
        return False
    
    print(f"‚úÖ ScraperAPI key found: {api_key[:10]}...{api_key[-4:]}")
    
    # Test basic ScraperAPI connection
    print("\nüîç Testing ScraperAPI connection...")
    
    async def test_connection():
        try:
            # Test request to httpbin.org through ScraperAPI
            params = {
                'api_key': api_key,
                'url': 'https://httpbin.org/ip',
                'country_code': 'us',
                'premium': True
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get('https://api.scraperapi.com', params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        print(f"‚úÖ ScraperAPI connection successful!")
                        print(f"üìç IP through ScraperAPI: {data.get('origin', 'Unknown')}")
                        return True
                    else:
                        print(f"‚ùå ScraperAPI connection failed: HTTP {response.status}")
                        return False
                        
        except Exception as e:
            print(f"‚ùå ScraperAPI connection error: {e}")
            return False
    
    # Run test
    success = asyncio.run(test_connection())
    
    if success:
        print("\nüéØ Testing advanced features...")
        
        async def test_advanced_features():
            try:
                # Test JavaScript rendering
                params = {
                    'api_key': api_key,
                    'url': 'https://httpbin.org/headers',
                    'render': True,
                    'country_code': 'de',
                    'premium': True
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.get('https://api.scraperapi.com', params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            print(f"‚úÖ JavaScript rendering successful!")
                            print(f"üåç Request from Germany: {data.get('headers', {}).get('X-Forwarded-For', 'Unknown')}")
                            return True
                        else:
                            print(f"‚ö†Ô∏è JavaScript rendering test failed: HTTP {response.status}")
                            return False
                            
            except Exception as e:
                print(f"‚ö†Ô∏è Advanced features test error: {e}")
                return False
        
        asyncio.run(test_advanced_features())
        
        print("\nüìä Account information:")
        print("üìà Check your dashboard: https://dashboard.scraperapi.com")
        print("üìö Documentation: https://docs.scraperapi.com")
        print("üí∞ Pricing: https://www.scraperapi.com/pricing/")
        
        print("\nüéØ Recommended plans for dark web intelligence:")
        print("‚Ä¢ Startup: $50/month - 100,000 requests")
        print("‚Ä¢ Business: $150/month - 300,000 requests") 
        print("‚Ä¢ Scale: $500/month - 1,000,000 requests")
        
        print("\n‚úÖ ScraperAPI setup complete!")
        return True
    else:
        return False

def create_env_file():
    """Create .env file with ScraperAPI key"""
    env_file = ".env"
    
    if not os.path.exists(env_file):
        print(f"\nüìù Creating {env_file} file...")
        
        api_key = input("Enter your ScraperAPI key (or press Enter to skip): ").strip()
        
        if api_key:
            with open(env_file, 'w') as f:
                f.write(f"# ScraperAPI Configuration\n")
                f.write(f"SCRAPERAPI_KEY={api_key}\n")
                f.write(f"# Dark Web Intelligence Collection\n")
                f.write(f"SCRAPERAPI_RATE_LIMIT=10\n")
                f.write(f"SCRAPERAPI_PREMIUM=true\n")
            
            print(f"‚úÖ {env_file} file created with your ScraperAPI key")
            print("üîê Your API key is now configured for the dark web collector")
        else:
            print("‚ö†Ô∏è No API key provided. You can set it later with:")
            print("export SCRAPERAPI_KEY='your_api_key_here'")
    else:
        print(f"‚úÖ {env_file} file already exists")

def test_dark_web_collector():
    """Test the ScraperAPI dark web collector"""
    print("\nüåë Testing ScraperAPI Dark Web Intelligence Collector...")
    
    api_key = os.getenv('SCRAPERAPI_KEY')
    if not api_key:
        print("‚ùå ScraperAPI key required for testing")
        return False
    
    try:
        # Import and test the collector
        from src.collectors.scraperapi_dark_web_collector import ScraperAPIDarkWebCollector
        
        async def test_collector():
            async with ScraperAPIDarkWebCollector(api_key=api_key, max_workers=2) as collector:
                # Test with a few sources
                test_sources = {
                    'malware_traffic': {
                        'url': 'https://www.malware-traffic-analysis.net/',
                        'type': 'blog',
                        'priority': 'high',
                        'scraperapi_config': {
                            'country_code': 'us',
                            'render': False,
                            'premium': True
                        }
                    }
                }
                
                # Override sources for testing
                collector.dark_web_sources = test_sources
                
                print("üîç Collecting test intelligence...")
                items = await collector.collect_all_dark_web_intelligence()
                
                print(f"‚úÖ Test collection complete: {len(items)} items collected")
                
                if items:
                    print("\nüìä Sample intelligence:")
                    for i, item in enumerate(items[:3]):
                        print(f"\n{i+1}. {item.title[:50]}...")
                        print(f"   Source: {item.source}")
                        print(f"   Threat Type: {item.threat_type}")
                        print(f"   Urgency: {item.urgency_level}")
                        print(f"   IOCs: {len(item.iocs)}")
                        print(f"   Country: {item.proxy_country}")
                
                return len(items) > 0
        
        success = asyncio.run(test_collector())
        
        if success:
            print("\nüéØ ScraperAPI dark web collector is working perfectly!")
            print("üöÄ Ready for full-scale intelligence collection")
        else:
            print("\n‚ö†Ô∏è Test completed but no items collected")
            print("üîç This may be normal depending on source availability")
        
        return success
        
    except Exception as e:
        print(f"‚ùå Error testing dark web collector: {e}")
        return False

def show_usage_examples():
    """Show usage examples for ScraperAPI dark web collector"""
    print("\nüìö ScraperAPI Dark Web Collector Usage Examples")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ Basic Collection:")
    print("export SCRAPERAPI_KEY='your_key'")
    print("python3 src/collectors/scraperapi_dark_web_collector.py")
    
    print("\n2Ô∏è‚É£ Custom Configuration:")
    print("python3 -c \"")
    print("import asyncio")
    print("from src.collectors.scraperapi_dark_web_collector import ScraperAPIDarkWebCollector")
    print("async def main():")
    print("    async with ScraperAPIDarkWebCollector('your_key') as collector:")
    print("        items = await collector.collect_all_dark_web_intelligence()")
    print("        print(f'Collected {len(items)} intelligence items')")
    print("asyncio.run(main())")
    print("\"")
    
    print("\n3Ô∏è‚É£ Integration with Existing Pipeline:")
    print("# Add to your unified collector")
    print("dark_web_items = await scraperapi_collector.collect_all_dark_web_intelligence()")
    print("all_intelligence.extend(dark_web_items)")
    
    print("\n4Ô∏è‚É£ Custom Source Configuration:")
    print("custom_sources = {")
    print("    'my_forum': {")
    print("        'url': 'https://example-forum.com/',")
    print("        'type': 'forum',")
    print("        'scraperapi_config': {")
    print("            'country_code': 'us',")
    print("            'render': True,")
    print("            'premium': True")
    print("        }")
    print("    }")
    print("}")
    
    print("\n5Ô∏è‚É£ Advanced Features:")
    print("# Enable JavaScript rendering")
    print("# Use ultra-premium proxies")
    print("# Rotate through multiple countries")
    print("# Extract comprehensive IOCs")
    print("# Generate credibility scores")

def show_best_practices():
    """Show ScraperAPI best practices for dark web intelligence"""
    print("\nüéØ ScraperAPI Best Practices for Dark Web Intelligence")
    print("=" * 60)
    
    print("\nüîß Configuration Best Practices:")
    print("‚Ä¢ Use premium proxies for better success rates")
    print("‚Ä¢ Enable JavaScript rendering for modern sites")
    print("‚Ä¢ Rotate countries to avoid geographic blocking")
    print("‚Ä¢ Set appropriate rate limits to respect targets")
    print("‚Ä¢ Use retry logic with different configurations")
    
    print("\nüåç Geographic Targeting:")
    print("‚Ä¢ US: General access, good for most sites")
    print("‚Ä¢ DE: Excellent for European forums")
    print("‚Ä¢ GB: Good for UK-based communities")
    print("‚Ä¢ NL: More liberal content policies")
    print("‚Ä¢ FR: Good for French-speaking sources")
    
    print("\n‚ö° Performance Optimization:")
    print("‚Ä¢ Limit concurrent requests to avoid overload")
    print("‚Ä¢ Use connection pooling for efficiency")
    print("‚Ä¢ Implement exponential backoff for retries")
    print("‚Ä¢ Monitor credit usage to control costs")
    print("‚Ä¢ Cache results when appropriate")
    
    print("\nüõ°Ô∏è Security Considerations:")
    print("‚Ä¢ Rotate user agents randomly")
    print("‚Ä¢ Use realistic headers")
    print("‚Ä¢ Implement delays between requests")
    print("‚Ä¢ Monitor for anti-scraping measures")
    print("‚Ä¢ Respect robots.txt when appropriate")
    
    print("\nüí∞ Cost Management:")
    print("‚Ä¢ Monitor daily credit usage")
    print("‚Ä¢ Prioritize high-value sources")
    print("‚Ä¢ Use smart retry to avoid wasted credits")
    print("‚Ä¢ Set usage alerts in dashboard")
    print("‚Ä¢ Consider higher plans for scale")

def main():
    """Main setup function"""
    print("üåë ScraperAPI Dark Web Intelligence Collection Setup")
    print("=" * 60)
    print("Professional-grade proxy rotation for underground threat intelligence")
    
    # Setup steps
    if not setup_scraperapi():
        print("\n‚ùå ScraperAPI setup failed. Please check your API key.")
        return
    
    create_env_file()
    
    # Test collector
    test_success = test_dark_web_collector()
    
    # Show additional information
    show_usage_examples()
    show_best_practices()
    
    print("\nüéâ Setup Complete!")
    print("=" * 60)
    
    if test_success:
        print("‚úÖ ScraperAPI is configured and working")
        print("üöÄ Ready for dark web intelligence collection")
        print("üìä Monitor usage at: https://dashboard.scraperapi.com")
    else:
        print("‚ö†Ô∏è Basic setup complete, but collector test failed")
        print("üîç Check source availability and network connectivity")
    
    print("\nüìñ Next Steps:")
    print("1. Run full collection: python3 src/collectors/scraperapi_dark_web_collector.py")
    print("2. Monitor results in data/raw/ directory")
    print("3. Integrate with your existing pipeline")
    print("4. Customize sources for your specific needs")
    print("5. Set up automated collection scheduling")

if __name__ == "__main__":
    main()
